import streamlit as st
import json
import hashlib
import shutil
from pathlib import Path
from datetime import datetime
from utils.data_utils import DataLoader
from utils import config


def get_file_hash(path: Path) -> str:
    """–ü–æ–ª—É—á–∞–µ—Ç —Ö–µ—à —Ñ–∞–π–ª–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
    h = hashlib.sha256()
    with open(path, "rb") as f:
        while chunk := f.read(8192):
            h.update(chunk)
    return h.hexdigest()


def get_workflow_cache_path(source_path: Path) -> Path:
    """–ü—É—Ç—å –¥–æ —Ñ–∞–π–ª–∞ workflow –∫–µ—à–∞"""
    safe_name = source_path.stem.replace(" ", "_")
    cache_dir = Path("workflows") / safe_name
    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir / "workflow.json"


def create_export_package(source_path: Path, workflow_path: Path, analyze_cache_path: Path) -> Path:
    """–°–æ–∑–¥–∞–µ—Ç –ø–∞–ø–∫—É —ç–∫—Å–ø–æ—Ä—Ç–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π –∏ –∫–æ–ø–∏—Ä—É–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã"""
    # –°–æ–∑–¥–∞–µ–º –∏–º—è –ø–∞–ø–∫–∏ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_name = source_path.stem.replace(" ", "_")
    export_dir = Path("exports") / f"{safe_name}_{timestamp}"
    export_dir.mkdir(parents=True, exist_ok=True)
    
    # –ö–æ–ø–∏—Ä—É–µ–º parquet —Ñ–∞–π–ª
    parquet_path = st.session_state.loader.get_parquet_path(source_path)
    if parquet_path.exists():
        shutil.copy2(parquet_path, export_dir / f"{safe_name}.parquet")
    
    # –ö–æ–ø–∏—Ä—É–µ–º workflow —Ñ–∞–π–ª
    if workflow_path.exists():
        shutil.copy2(workflow_path, export_dir / "workflow.json")
    
    # –ö–æ–ø–∏—Ä—É–µ–º analyze —Ñ–∞–π–ª
    if analyze_cache_path.exists():
        shutil.copy2(analyze_cache_path, export_dir / "columns_data.json")
    
    return export_dir


def step_save_workflow():
    """8-–π —à–∞–≥: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ workflow –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞"""
    if "lazy_df" not in st.session_state or st.session_state.lazy_df is None:
        st.error("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –Ω–∞ —à–∞–≥–µ 1!")
        return

    if "columns_data" not in st.session_state or not st.session_state.columns_data:
        st.error("–°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª –Ω–∞ —à–∞–≥–µ 2!")
        return

    st.subheader("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ Workflow")
    st.info("üí° –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –Ω–∏–∂–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ –≤ workflow —Ñ–∞–π–ª.")

    # –ü—Ä–æ—Å—Ç–∞—è –∫–Ω–æ–ø–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    if st.button("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å Workflow", type="primary"):
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ
            source_path = st.session_state.source_file
            file_hash = get_file_hash(source_path)
            parquet_path = st.session_state.loader.get_parquet_path(source_path)
            
            # –°–æ–±–∏—Ä–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–ª–æ–Ω–æ–∫
            columns_data = st.session_state.columns_data
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º—ã –∫–æ–ª–æ–Ω–æ–∫
            standalone_columns = [
                col for col, data in columns_data.items()
                if data.get("mode") == "standalone"
            ]
            
            exclude_columns = [
                col for col, data in columns_data.items()
                if data.get("mode") == "exclude"
            ]
            
            # –°–æ–±–∏—Ä–∞–µ–º display names
            display_names = {
                col: data.get("display_name", col)
                for col, data in columns_data.items()
                if data.get("mode") == "standalone"
            }
            
            # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏–∏
            concatenations = []
            for col, data in columns_data.items():
                if data.get("concatenated"):
                    concat_info = data["concatenated"]
                    concatenations.append({
                        "name": col,
                        "source_columns": concat_info["source_columns"],
                        "separator": concat_info["separator"]
                    })
            
            # –°–æ–±–∏—Ä–∞–µ–º regex –ø—Ä–∞–≤–∏–ª–∞
            regex_rules = {}
            for col, data in columns_data.items():
                if data.get("selected_patterns"):
                    regex_rules[col] = data["selected_patterns"]
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —ç–∫—Å–ø–æ—Ä—Ç–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            export_settings = {
                "format": "parquet",
                "parquet": {
                    "compression": "zstd",
                    "target_mb_per_file": 100
                },
                "csv": {
                    "delimiter": "~",
                    "quote_all": True
                },
                "output_dir": f"exports/{source_path.stem}"
            }
            
            # –°–æ–∑–¥–∞–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é workflow —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            workflow = {
                "version": "1.0",
                "created_at": datetime.now().isoformat(),
                "source": {
                    "parquet_path": str(parquet_path),
                    "file_hash": file_hash,
                    "schema": list(st.session_state.lazy_df.collect_schema().names())
                },
                "columns": {
                    "standalone": standalone_columns,
                    "exclude": exclude_columns
                },
                "display_names": display_names,
                "concatenations": concatenations,
                "regex_rules": regex_rules,
                "dedup": {
                    "unique_columns": st.session_state.dedup_settings.get("unique_columns", [])
                },
                "not_empty": {
                    "columns": st.session_state.dedup_settings.get("not_empty_columns", [])
                },
                "export": export_settings
            }

            cache_path = get_workflow_cache_path(source_path)
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump(workflow, f, ensure_ascii=False, indent=2)
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É —ç–∫—Å–ø–æ—Ä—Ç–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
            parquet_path = st.session_state.loader.get_parquet_path(source_path)
            analyze_cache_path = Path("analyze_cache") / source_path.stem / "columns_data.json"
            export_dir = create_export_package(source_path, cache_path, analyze_cache_path)
            
            st.success(f"‚úÖ Workflow —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {cache_path}")
            st.success(f"üì¶ –ü–∞–∫–µ—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞ —Å–æ–∑–¥–∞–Ω: {export_dir}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º CLI –∫–æ–º–∞–Ω–¥—É —Å –Ω–æ–≤—ã–º–∏ –ø—É—Ç—è–º–∏
            cli_command = f"""python cli_process.py \\
    --path {export_dir / f"{source_path.stem}.parquet"} \\
    --analyze_cache {export_dir / "columns_data.json"} \\
    --workflow {export_dir / "workflow.json"}"""
            
            st.code(cli_command, language="bash")
            st.info("üí° –°–∫–æ–ø–∏—Ä—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É –¥–ª—è –∑–∞–ø—É—Å–∫–∞ CLI –ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞")

        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
