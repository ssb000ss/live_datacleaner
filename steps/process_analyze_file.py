import logging
import json
import hashlib
from pathlib import Path
import gc

import streamlit as st
import polars as pl

from utils.data_cleaner import PatternDetector, short_hash
from utils.ui_utils import show_table
from utils import config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(config.APP_TITLE)

CACHE_ROOT = Path("analyze_cache")


def analyze_file():
    if "lazy_df" not in st.session_state or st.session_state.lazy_df is None:
        st.error("–î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –∫ —à–∞–≥—É 1.")
        return

    st.markdown("# –ê–Ω–∞–ª–∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    show_table()

    # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –ø–∞–ø–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
    if "columns_data" in st.session_state and st.session_state.columns_data:
        st.success("‚úÖ –î–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–æ–∫ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ —Ñ–∞–π–ª–∞!")
        st.info("–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ columns_data.json")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–æ–Ω–∫–∞—Ö
        if st.session_state.columns_data:
            st.markdown("### –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–ª–æ–Ω–∫–∞—Ö:")
            for col, data in st.session_state.columns_data.items():
                with st.expander(f"üìä {col}"):
                    st.json(data)
        return

    st.checkbox("–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–µ—à", key="ignore_column_cache", value=False)

    if st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏"):
        lazy_df = st.session_state.lazy_df
        source_path = st.session_state.source_file

        file_hash = get_file_hash(source_path)
        cache_path = get_cache_path(source_path)

        if not st.session_state.ignore_column_cache:
            cached = try_load_cached_columns_data(lazy_df, file_hash, cache_path)
            if cached:
                st.session_state.columns_data = cached["columns_data"]
                st.success("‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–ª–æ–Ω–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫–µ—à–∞.")
                return

        if lazy_df is not None:
            success = analyze_columns(lazy_df, st.session_state.pattern_detector, source_path)
            if success:
                save_columns_data(st.session_state.columns_data, file_hash, cache_path)
                st.success("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã!")
        else:
            st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ.")


def analyze_columns(
        lazy_df: pl.LazyFrame,
        pattern_detector: PatternDetector,
        source_path: Path,
) -> bool:
    columns_data = {}

    logger.info(f"Starting column analysis for file: {source_path}")
    try:
        if lazy_df is None:
            st.error("–î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return
            
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –±–µ–∑ materialize –≤—Å–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
        column_names = list(lazy_df.collect_schema().names())
        total_columns = len(column_names)
        progress_bar = st.progress(0)

        for idx, column in enumerate(column_names, start=1):
            logger.info(f"Analyzing column '{column}' in file: {source_path.name}")

            # –§–æ—Ä–º–∏—Ä—É–µ–º –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–ª–∏—á–∏—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –≤ –∫–æ–ª–æ–Ω–∫–µ
            try:
                exprs = [
                    pl.col(column).cast(pl.Utf8).str.contains(pattern, literal=False).any().alias(name)
                    for name, pattern in pattern_detector.regex_patterns.items()
                ]
                # –°–æ–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–∫–∞–ª—è—Ä–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (True/False –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞)
                result_df = lazy_df.select(exprs).collect()
                detected_patterns = {name for name in result_df.columns if bool(result_df[0, name])}
            except Exception as e:
                logger.error(f"Failed pattern detection for column '{column}': {e}")
                detected_patterns = set()

            sorted_patterns = sorted(detected_patterns)
            columns_data[column] = {
                "hash": short_hash(column),
                "column": column,
                "origin_name": column,
                "display_name": column,
                "detected_patterns": sorted_patterns,
                "prev_selected_patterns": sorted_patterns.copy(),
                "selected_patterns": sorted_patterns.copy(),
                "detected_display_patterns": [
                    config.PATTERN_DISPLAY_MAP_UNICODE.get(p, p) for p in sorted_patterns
                ],
                "display_patterns": [
                    config.PATTERN_DISPLAY_MAP_UNICODE.get(p, p) for p in sorted_patterns
                ],
                "mode": "standalone",
                "concatenated": None,
            }

            # –Ø–≤–Ω–æ –æ—Å–≤–æ–±–æ–∂–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –∏ —á–∏—Å—Ç–∏–º GC, —á—Ç–æ–±—ã –Ω–µ –Ω–∞–∫–∞–ø–ª–∏–≤–∞—Ç—å –ø–∞–º—è—Ç—å –Ω–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–∞—Ö
            del result_df
            gc.collect()

            progress_bar.progress(idx / total_columns)

    except Exception as e:
        logger.error("Error during column analysis:", exc_info=True)
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∫–æ–ª–æ–Ω–æ–∫: {e}")
        return False

    st.session_state.columns_data = columns_data
    logger.info("Column analysis completed. Column data saved to session_state.")

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫
    del columns_data
    gc.collect()
    return True


def get_file_hash(path: Path) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        while chunk := f.read(8192):
            h.update(chunk)
    return h.hexdigest()


def get_cache_path(source_path: Path) -> Path:
    """–ü—É—Ç—å –¥–æ —Ñ–∞–π–ª–∞ –∫–µ—à–∞: cache/<–∏–º—è_—Ñ–∞–π–ª–∞>/columns_data.json"""
    safe_name = source_path.stem.replace(" ", "_")
    cache_dir = CACHE_ROOT / safe_name
    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir / "columns_data.json"


def try_load_cached_columns_data(lazy_df: pl.LazyFrame, file_hash: str, cache_path: Path) -> dict | None:
    if not cache_path.exists():
        return None

    try:
        with open(cache_path, "r", encoding="utf-8") as f:
            cache = json.load(f)

        cached_columns = set(cache.get("columns_data", {}).keys())
        current_columns = set(lazy_df.collect_schema().names())

        if cache.get("file_hash") == file_hash and cached_columns == current_columns:
            logger.info("–ó–∞–≥—Ä—É–∂–µ–Ω –∫–µ—à –∫–æ–ª–æ–Ω–æ–∫ —Å —Å–æ–≤–ø–∞–¥–∞—é—â–∏–º —Ö—ç—à–µ–º —Ñ–∞–π–ª–∞.")
            return cache
        else:
            logger.info("–•—ç—à –∏–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–ª–æ–Ω–æ–∫ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç ‚Äî –∫–µ—à –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è.")

    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫–µ—à–∞: {e}")

    return None


def save_columns_data(columns_data: dict, file_hash: str, cache_path: Path):
    try:
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump({
                "file_hash": file_hash,
                "columns_data": columns_data
            }, f, ensure_ascii=False, indent=2)
        logger.info(f"–°–æ—Ö—Ä–∞–Ω—ë–Ω –∫–µ—à columns_data: {cache_path}")
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–µ—à–∞: {e}")
