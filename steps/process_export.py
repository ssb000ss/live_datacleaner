import os
import csv
import logging
from pathlib import Path

import streamlit as st
import polars as pl

from utils import config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(config.APP_TITLE)


def rename_columns(ldf: pl.LazyFrame) -> pl.LazyFrame:
    if "columns_data" not in st.session_state:
        raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –∫–æ–ª–æ–Ω–∫–∞—Ö –≤ session_state.columns_data")

    columns_data = st.session_state.columns_data

    included_columns = [
        col for col, data in columns_data.items()
        if data.get("mode") != "exclude" and col in ldf.collect_schema()
    ]

    # –§–æ—Ä–º–∏—Ä—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∏–º–µ–Ω–∞ —Å—Ç–æ–ª–±—Ü–æ–≤ —Å —É—á—ë—Ç–æ–º –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è display_name –∏ –¥—É–±–ª–µ–π
    safe_names = []
    used = set()
    for col in included_columns:
        base_name = columns_data.get(col, {}).get("display_name", col) or col
        name = base_name
        idx = 2
        # –æ–±–µ—Å–ø–µ—á–∏–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –∏–º—ë–Ω
        while name in used:
            name = f"{base_name}_{idx}"
            idx += 1
        used.add(name)
        safe_names.append((col, name))

    rename_map = {orig: new for orig, new in safe_names}

    renamed_ldf = ldf.select(included_columns).rename(rename_map)

    return renamed_ldf


def step_export_file():
    if "lazy_df" not in st.session_state or st.session_state.lazy_df is None:
        st.error("–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞. –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —à–∞–≥ 1.")
        return

    if "source_file" not in st.session_state or st.session_state.source_file is None:
        st.error("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ –∏–º—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞.")
        return

    st.subheader("üì§ –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö")

    export_format = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞:", ["parquet", "csv"])
    base_export_dir = st.text_input("–ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞:", value="exports")

    if export_format == "parquet":
        max_file_size_mb = st.number_input(
            "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ (–ú–ë):", min_value=10, max_value=1000, value=100, step=10
        )
        compression = st.selectbox("–°–∂–∞—Ç–∏–µ Parquet:", ["zstd", "snappy", "gzip", "none"], index=0)
    elif export_format == "csv":
        csv_delimiter = st.text_input("–†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å CSV:", value="|")
        csv_quote_all = st.checkbox("–ó–∞–∫–ª—é—á–∞—Ç—å –≤—Å–µ –ø–æ–ª—è –≤ –∫–∞–≤—ã—á–∫–∏", value=True)

    if st.button("–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å"):
        try:
            original_name = st.session_state.source_file.stem
            target_dir = Path(base_export_dir) / original_name
            target_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"–≠–∫—Å–ø–æ—Ä—Ç–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {target_dir.resolve()}")

            # ‚ö†Ô∏è –í—Å–µ –¥–µ–π—Å—Ç–≤–∏—è –≤ –ª–µ–Ω–∏–≤–æ–º —Ä–µ–∂–∏–º–µ –î–û collect()
            ldf = rename_columns(st.session_state.lazy_df)

            st.info("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –ø–∞–º—è—Ç—å...")
            df = ldf.collect()
            st.write(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {df.height}, –∫–æ–ª–æ–Ω–æ–∫: {df.width}")

            if export_format == "parquet":
                rows_per_chunk = estimate_rows_per_chunk(df, max_file_size_mb)
                num_chunks = (df.height + rows_per_chunk - 1) // rows_per_chunk
                st.write(f"üî¢ –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤: {num_chunks} (–ø–æ {rows_per_chunk} —Å—Ç—Ä–æ–∫)")

                for i in range(num_chunks):
                    chunk = df.slice(i * rows_per_chunk, rows_per_chunk)
                    filename = target_dir / f"{original_name}_part_{i + 1}.parquet"
                    chunk.write_parquet(
                        filename,
                        compression=None if compression == "none" else compression
                    )
                    logger.info(f"–ó–∞–ø–∏—Å–∞–Ω: {filename.name}")
                st.success(f"‚úÖ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {num_chunks} .parquet —Ñ–∞–π–ª–æ–≤ –≤ `{target_dir}`")

            elif export_format == "csv":
                filename = target_dir / f"{original_name}.csv"

                # –ó–∞–º–µ–Ω—è–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫ –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª–µ–π, —á—Ç–æ–±—ã —Å—Ç—Ä–æ–∫–∏ –Ω–µ –ø–µ—Ä–µ–Ω–æ—Å–∏–ª–∏—Å—å
                try:
                    string_cols = [col for col, dtype in df.schema.items() if dtype == pl.Utf8]
                except AttributeError:
                    # Fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π polars
                    string_cols = [col for col in df.columns if pl.datatypes.is_utf8(df[col].dtype)]

                if string_cols:
                    df = df.with_columns([
                        pl.col(c)
                        .str.replace_all("\r\n", " ")
                        .str.replace_all("\n", " ")
                        .str.replace_all("\r", " ")
                        .alias(c)
                        for c in string_cols
                    ])

                # –ü–∏—à–µ–º CSV –≤—Ä—É—á–Ω—É—é, —á—Ç–æ–±—ã –æ–±–µ—Å–ø–µ—á–∏—Ç—å –Ω—É–∂–Ω—ã–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –∏ –∫–∞–≤—ã—á–∫–∏
                with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
                    writer = csv.writer(
                        f,
                        delimiter=csv_delimiter,
                        quoting=csv.QUOTE_ALL if csv_quote_all else csv.QUOTE_MINIMAL,
                        lineterminator='\n'
                    )
                    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                    writer.writerow(df.columns)
                    # –°—Ç—Ä–æ–∫–∏
                    for row in df.iter_rows():
                        safe_row = ["" if v is None else v for v in row]
                        writer.writerow(safe_row)
                st.success(f"‚úÖ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ CSV: `{filename}`")

        except Exception as e:
            logger.exception("–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ")
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ: {e}")


def estimate_rows_per_chunk(df: pl.DataFrame, target_mb: int) -> int:
    try:
        sample_size = min(10_000, df.height)
        sample = df.head(sample_size)

        sample_path = "/tmp/sample_export.parquet"
        sample.write_parquet(sample_path, compression="zstd")
        size_mb = os.path.getsize(sample_path) / 1024 / 1024
        os.remove(sample_path)

        if size_mb == 0:
            return max(df.height // 4, 100_000)  # –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –¥–µ—Ñ–æ–ª—Ç

        rows_per_mb = sample_size / size_mb
        return max(int(rows_per_mb * target_mb), 10_000)  # –Ω–µ –º–µ–Ω—å—à–µ 10k

    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ü–µ–Ω–∏—Ç—å —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {e}")
        return max(df.height // 4, 100_000)  # –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç
